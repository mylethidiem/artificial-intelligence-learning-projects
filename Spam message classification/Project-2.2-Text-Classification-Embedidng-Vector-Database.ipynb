{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdE3415GA72R"
   },
   "source": [
    "## **0. Tải bộ dữ liệu**\n",
    "**Lưu ý:** Nếu bạn không thể sử dụng lệnh gdown để tải bộ dữ liệu vì bị giới hạn số lượt tải, hãy tải bộ dữ liệu thử công và upload lên google drive của mình. Sau đó, sử dụng lệnh dưới đây để copy file dữ liệu vào colab:\n",
    "```python\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "!cp /path/to/dataset/on/your/drive .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "urzyheIkVQFv",
    "outputId": "45677693-9349-4028-9922-90d5a82afd04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.5.1 requires sympy==1.13.1; python_version >= \"3.9\", but you have sympy 1.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qq faiss-cpu\n",
    "!pip install -qq transformers\n",
    "!pip install -qq pandas\n",
    "!pip install -qq numpy\n",
    "!pip install -qq scikit-learn\n",
    "!pip install -qq tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KHfGc0o2fZMu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/banhmuy/miniconda3/envs/ds_env/lib/python3.11/site-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1N7rk-kfnDFIGMeX0ROVTjKh71gcgx-7R\n",
      "To: /home/banhmuy/artificial-intelligence-learning-projects/Spam message classification/2cls_spam_text_cls.csv\n",
      "100%|████████████████████████████████████████| 486k/486k [00:00<00:00, 2.13MB/s]\n"
     ]
    }
   ],
   "source": [
    "# https://drive.google.com/file/d/1N7rk-kfnDFIGMeX0ROVTjKh71gcgx-7R/view?usp=sharing\n",
    "!gdown --id 1N7rk-kfnDFIGMeX0ROVTjKh71gcgx-7R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVvC6obsBYG4"
   },
   "source": [
    "## **1. Import các thư viện cần thiết**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tU6nV0YHhWWm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/banhmuy/miniconda3/envs/ds_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vX0MeDrnBcY1"
   },
   "source": [
    "## **2. Đọc bộ dữ liệu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KoeUNKOclJK9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category                                            Message\n",
       "0         ham  Go until jurong point, crazy.. Available only ...\n",
       "1         ham                      Ok lar... Joking wif u oni...\n",
       "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         ham  U dun say so early hor... U c already then say...\n",
       "4         ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...       ...                                                ...\n",
       "5567     spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568      ham               Will ü b going to esplanade fr home?\n",
       "5569      ham  Pity, * was in mood for that. So...any other s...\n",
       "5570      ham  The guy did some bitching but I acted like i'd...\n",
       "5571      ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATASET_PATH = '/content/2cls_spam_text_cls.csv'\n",
    "DATASET_PATH = '2cls_spam_text_cls.csv'\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "uQblTvxTnIPj"
   },
   "outputs": [],
   "source": [
    "messages = df['Message'].values.tolist()\n",
    "labels = df['Category'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       " 'Ok lar... Joking wif u oni...',\n",
       " \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
       " 'U dun say so early hor... U c already then say...',\n",
       " \"Nah I don't think he goes to usf, he lives around here though\",\n",
       " \"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\",\n",
       " 'Even my brother is not like to speak with me. They treat me like aids patent.',\n",
       " \"As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\",\n",
       " 'WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.',\n",
       " 'Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ham', 'ham', 'spam', 'ham', 'ham', 'spam', 'ham', 'ham', 'spam', 'spam']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WadqE8dtBf9Q"
   },
   "source": [
    "## **3. Chuẩn bị embedding model và dữ liệu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2FJWXIABiwa"
   },
   "source": [
    "### **3.1. Load embedding model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xzBrlBKslOz5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model loaded: intfloat/multilingual-e5-base\n"
     ]
    }
   ],
   "source": [
    "# Load embedding model\n",
    "MODEL_NAME = 'intfloat/multilingual-e5-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f'Using device: {device}')\n",
    "print(f'Model loaded: {MODEL_NAME}')\n",
    "\n",
    "def average_pool(last_hidden_states, attention_mask):\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7aTeSvzBogY"
   },
   "source": [
    "### **3.2. Tạo sentence embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Q_m5jsZBlh5P"
   },
   "outputs": [],
   "source": [
    "def get_embeddings(texts, model, tokenizer, device, batch_size=32):\n",
    "    \"\"\"Generate embeddings for a list of texts\"\"\"\n",
    "    embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "\n",
    "        # Add passage prefix for better retrieval performance\n",
    "        batch_texts_with_prefix = [f\"passage: {text}\" for text in batch_texts]\n",
    "\n",
    "        # Tokenize\n",
    "        batch_dict = tokenizer(batch_texts_with_prefix,\n",
    "                              max_length=512,\n",
    "                              padding=True,\n",
    "                              truncation=True,\n",
    "                              return_tensors='pt')\n",
    "\n",
    "        # Move to device\n",
    "        batch_dict = {k: v.to(device) for k, v in batch_dict.items()}\n",
    "\n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch_dict)\n",
    "            batch_embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "            # Normalize embeddings\n",
    "            batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1)\n",
    "            embeddings.append(batch_embeddings.cpu().numpy())\n",
    "\n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Z5KV1Kpwmh7y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['ham' 'spam']\n",
      "Generating embeddings for 5572 messages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 175/175 [07:26<00:00,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (5572, 768)\n",
      "Created metadata for 5572 documents\n",
      "Created metadata for 5572 documents\n"
     ]
    }
   ],
   "source": [
    "# Prepare labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "print(f'Classes: {le.classes_}')\n",
    "\n",
    "# Generate embeddings for all messages\n",
    "print(f\"Generating embeddings for {len(messages)} messages...\")\n",
    "X_embeddings = get_embeddings(messages, model, tokenizer, device)\n",
    "print(f\"Embeddings shape: {X_embeddings.shape}\")\n",
    "\n",
    "# Create metadata for each document\n",
    "metadata = []\n",
    "for i, (message, label) in enumerate(zip(messages, labels)):\n",
    "    metadata.append({\n",
    "        'index': i,\n",
    "        'message': message,\n",
    "        'label': label,\n",
    "        'label_encoded': y[i]\n",
    "    })\n",
    "\n",
    "print(f\"Created metadata for {len(metadata)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ti67ymhoVQFz"
   },
   "source": [
    "### **3.3. Tạo FAISS index và chia dữ liệu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "tEy3OrleoARz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 5014\n",
      "Test size: 558\n",
      "Train label distribution: [4342  672]\n",
      "Test label distribution: [483  75]\n",
      "FAISS index created with 5014 vectors\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test (90% train, 10% test)\n",
    "TEST_SIZE = 0.1\n",
    "SEED = 42\n",
    "\n",
    "train_indices, test_indices = train_test_split(\n",
    "    range(len(messages)),\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=y,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Split embeddings and metadata\n",
    "X_train_emb = X_embeddings[train_indices]\n",
    "X_test_emb = X_embeddings[test_indices]\n",
    "y_train = y[train_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "train_metadata = [metadata[i] for i in train_indices]\n",
    "test_metadata = [metadata[i] for i in test_indices]\n",
    "\n",
    "print(f\"Train size: {len(X_train_emb)}\")\n",
    "print(f\"Test size: {len(X_test_emb)}\")\n",
    "print(f\"Train label distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Test label distribution: {np.bincount(y_test)}\")\n",
    "\n",
    "# Create FAISS index\n",
    "embedding_dim = X_train_emb.shape[1]\n",
    "index = faiss.IndexFlatIP(embedding_dim)  # Inner product for cosine similarity\n",
    "index.add(X_train_emb.astype('float32'))\n",
    "\n",
    "print(f\"FAISS index created with {index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvzSFGPyB83P"
   },
   "source": [
    "## **4. Implement classification với embedding similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "QKm5qxIWhJmT"
   },
   "outputs": [],
   "source": [
    "def classify_with_knn(query_text, model, tokenizer, device, index, train_metadata, k=1):\n",
    "    \"\"\"Classify text using k-nearest neighbors with embeddings\"\"\"\n",
    "\n",
    "    # Get query embedding\n",
    "    query_with_prefix = f\"query: {query_text}\"\n",
    "    batch_dict = tokenizer([query_with_prefix],\n",
    "                          max_length=512,\n",
    "                          padding=True,\n",
    "                          truncation=True,\n",
    "                          return_tensors='pt')\n",
    "\n",
    "    batch_dict = {k: v.to(device) for k, v in batch_dict.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch_dict)\n",
    "        query_embedding = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "        query_embedding = F.normalize(query_embedding, p=2, dim=1)\n",
    "        query_embedding = query_embedding.cpu().numpy().astype('float32')\n",
    "\n",
    "    # Search in FAISS index\n",
    "    scores, indices = index.search(query_embedding, k)\n",
    "\n",
    "    # Get predictions from top-k neighbors\n",
    "    predictions = []\n",
    "    neighbor_info = []\n",
    "\n",
    "    for i in range(k):\n",
    "        neighbor_idx = indices[0][i]\n",
    "        neighbor_score = scores[0][i]\n",
    "        neighbor_label = train_metadata[neighbor_idx]['label']\n",
    "        neighbor_message = train_metadata[neighbor_idx]['message']\n",
    "\n",
    "        predictions.append(neighbor_label)\n",
    "        neighbor_info.append({\n",
    "            'score': float(neighbor_score),\n",
    "            'label': neighbor_label,\n",
    "            'message': neighbor_message[:100] + \"...\" if len(neighbor_message) > 100 else neighbor_message\n",
    "        })\n",
    "\n",
    "    # Majority vote for final prediction\n",
    "    unique_labels, counts = np.unique(predictions, return_counts=True)\n",
    "    final_prediction = unique_labels[np.argmax(counts)]\n",
    "\n",
    "    return final_prediction, neighbor_info\n",
    "\n",
    "def evaluate_knn_accuracy(test_embeddings, test_labels, test_metadata, index, train_metadata, k_values=[1, 3, 5]):\n",
    "    \"\"\"Evaluate accuracy for different k values using precomputed embeddings\"\"\"\n",
    "    results = {}\n",
    "    all_errors = {}\n",
    "\n",
    "    for k in k_values:\n",
    "        correct = 0\n",
    "        total = len(test_embeddings)\n",
    "        errors = []\n",
    "\n",
    "        for i in tqdm(range(total), desc=f\"Evaluating k={k}\"):\n",
    "            query_embedding = test_embeddings[i:i+1].astype('float32')\n",
    "            true_label = test_metadata[i]['label']\n",
    "            true_message = test_metadata[i]['message']\n",
    "\n",
    "            # Search in FAISS index\n",
    "            scores, indices = index.search(query_embedding, k)\n",
    "\n",
    "            # Get predictions from top-k neighbors\n",
    "            predictions = []\n",
    "            neighbor_details = []\n",
    "            for j in range(k):\n",
    "                neighbor_idx = indices[0][j]\n",
    "                neighbor_label = train_metadata[neighbor_idx]['label']\n",
    "                neighbor_message = train_metadata[neighbor_idx]['message']\n",
    "                neighbor_score = float(scores[0][j])\n",
    "\n",
    "                predictions.append(neighbor_label)\n",
    "                neighbor_details.append({\n",
    "                    'label': neighbor_label,\n",
    "                    'message': neighbor_message,\n",
    "                    'score': neighbor_score\n",
    "                })\n",
    "\n",
    "            # Majority vote\n",
    "            unique_labels, counts = np.unique(predictions, return_counts=True)\n",
    "            predicted_label = unique_labels[np.argmax(counts)]\n",
    "\n",
    "            if predicted_label == true_label:\n",
    "                correct += 1\n",
    "            else:\n",
    "                # Collect error information\n",
    "                error_info = {\n",
    "                    'index': i,\n",
    "                    'original_index': test_metadata[i]['index'],\n",
    "                    'message': true_message,\n",
    "                    'true_label': true_label,\n",
    "                    'predicted_label': predicted_label,\n",
    "                    'neighbors': neighbor_details,\n",
    "                    'label_distribution': {label: int(count) for label, count in zip(unique_labels, counts)}\n",
    "                }\n",
    "                errors.append(error_info)\n",
    "\n",
    "        accuracy = correct / total\n",
    "        error_count = total - correct\n",
    "\n",
    "        results[k] = accuracy\n",
    "        all_errors[k] = errors\n",
    "\n",
    "        print(f\"Accuracy with k={k}: {accuracy:.4f}\")\n",
    "        print(f\"Number of errors with k={k}: {error_count}/{total} ({(error_count/total)*100:.2f}%)\")\n",
    "\n",
    "    return results, all_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FTBzozgBuIx"
   },
   "source": [
    "## **5. Đánh giá accuracy trên test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "anE-mJw5qndx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating accuracy on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating k=1: 100%|██████████| 558/558 [00:00<00:00, 960.34it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with k=1: 0.9857\n",
      "Number of errors with k=1: 8/558 (1.43%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating k=3: 100%|██████████| 558/558 [00:00<00:00, 982.05it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with k=3: 0.9928\n",
      "Number of errors with k=3: 4/558 (0.72%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating k=5: 100%|██████████| 558/558 [00:00<00:00, 775.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with k=5: 0.9910\n",
      "Number of errors with k=5: 5/558 (0.90%)\n",
      "\n",
      "==================================================\n",
      "ACCURACY RESULTS\n",
      "==================================================\n",
      "Top-1 accuracy: 0.9857 (98.57%)\n",
      "Top-3 accuracy: 0.9928 (99.28%)\n",
      "Top-5 accuracy: 0.9910 (99.10%)\n",
      "==================================================\n",
      "\n",
      "***Error analysis saved to: error_analysis.json***\n",
      "\n",
      "***Summary:\n",
      "   k=1: 8 errors out of 558 samples\n",
      "   k=3: 4 errors out of 558 samples\n",
      "   k=5: 5 errors out of 558 samples\n",
      "CPU times: user 1.87 s, sys: 32.9 ms, total: 1.91 s\n",
      "Wall time: 1.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Evaluate accuracy for different k values\n",
    "print(\"Evaluating accuracy on test set...\")\n",
    "accuracy_results, error_results = evaluate_knn_accuracy(\n",
    "    X_test_emb,\n",
    "    y_test,\n",
    "    test_metadata,\n",
    "    index,\n",
    "    train_metadata,\n",
    "    k_values=[1, 3, 5]\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ACCURACY RESULTS\")\n",
    "print(\"=\"*50)\n",
    "for k, accuracy in accuracy_results.items():\n",
    "    print(f\"Top-{k} accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save error analysis to JSON file\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "error_analysis = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'model': MODEL_NAME,\n",
    "    'test_size': len(X_test_emb),\n",
    "    'accuracy_results': accuracy_results,\n",
    "    'errors_by_k': {}\n",
    "}\n",
    "\n",
    "for k, errors in error_results.items():\n",
    "    error_analysis['errors_by_k'][f'k_{k}'] = {\n",
    "        'total_errors': len(errors),\n",
    "        'error_rate': len(errors) / len(X_test_emb),\n",
    "        'errors': errors\n",
    "    }\n",
    "\n",
    "# Save to JSON file\n",
    "output_file = 'error_analysis.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(error_analysis, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n***Error analysis saved to: {output_file}***\")\n",
    "print()\n",
    "print(f\"***Summary:\")\n",
    "for k, errors in error_results.items():\n",
    "    print(f\"   k={k}: {len(errors)} errors out of {len(X_test_emb)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCoOf4BrBwYI"
   },
   "source": [
    "## **6. Pipeline classification cho user input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "t9qcQ_4Pqpes"
   },
   "outputs": [],
   "source": [
    "def spam_classifier_pipeline(user_input, k=3):\n",
    "    \"\"\"\n",
    "    Complete pipeline for spam classification\n",
    "\n",
    "    Args:\n",
    "        user_input (str): Text to classify\n",
    "        k (int): Number of nearest neighbors to consider\n",
    "\n",
    "    Returns:\n",
    "        dict: Classification results with details\n",
    "    \"\"\"\n",
    "\n",
    "    print()\n",
    "    print(f\"***Classifying: '{user_input}'\")\n",
    "    print()\n",
    "    print(f\"***Using top-{k} nearest neighbors\")\n",
    "    print()\n",
    "\n",
    "    # Get prediction and neighbors\n",
    "    prediction, neighbors = classify_with_knn(\n",
    "        user_input, model, tokenizer, device, index, train_metadata, k=k\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    print(f\"***Prediction: {prediction.upper()}\")\n",
    "    print()\n",
    "\n",
    "    print(\"***Top neighbors:\")\n",
    "    for i, neighbor in enumerate(neighbors, 1):\n",
    "        print(f\"{i}. Label: {neighbor['label']} | Score: {neighbor['score']:.4f}\")\n",
    "        print(f\"   Message: {neighbor['message']}\")\n",
    "        print()\n",
    "\n",
    "    # Count label distribution\n",
    "    labels = [n['label'] for n in neighbors]\n",
    "    label_counts = {label: labels.count(label) for label in set(labels)}\n",
    "\n",
    "    return {\n",
    "        'prediction': prediction,\n",
    "        'neighbors': neighbors,\n",
    "        'label_distribution': label_counts\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1GS4YWRByPB"
   },
   "source": [
    "## **7. Test pipeline với các ví dụ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "-BVohFfpqmJ8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pipeline with different examples:\n",
      "\n",
      "\n",
      "***Example 1:\n",
      "\n",
      "***Classifying: 'I am actually thinking a way of doing something useful'\n",
      "\n",
      "***Using top-3 nearest neighbors\n",
      "\n",
      "***Prediction: HAM\n",
      "\n",
      "***Top neighbors:\n",
      "1. Label: ham | Score: 0.8424\n",
      "   Message: yeah, that's what I was thinking\n",
      "\n",
      "2. Label: ham | Score: 0.8412\n",
      "   Message: that would be good … I'll phone you tomo lunchtime, shall I, to organise something?\n",
      "\n",
      "3. Label: ham | Score: 0.8344\n",
      "   Message: See? I thought it all through\n",
      "\n",
      "\n",
      "\n",
      "***Example 2:\n",
      "\n",
      "***Classifying: 'FREE!! Click here to win $1000 NOW! Limited time offer!'\n",
      "\n",
      "***Using top-3 nearest neighbors\n",
      "\n",
      "***Prediction: SPAM\n",
      "\n",
      "***Top neighbors:\n",
      "1. Label: spam | Score: 0.8567\n",
      "   Message: FREE MESSAGE Activate your 500 FREE Text Messages by replying to this message with the word FREE For...\n",
      "\n",
      "2. Label: spam | Score: 0.8567\n",
      "   Message: FREE MESSAGE Activate your 500 FREE Text Messages by replying to this message with the word FREE For...\n",
      "\n",
      "3. Label: spam | Score: 0.8489\n",
      "   Message: URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to ...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test với các ví dụ khác nhau\n",
    "test_examples = [\n",
    "    \"I am actually thinking a way of doing something useful\",\n",
    "    \"FREE!! Click here to win $1000 NOW! Limited time offer!\",\n",
    "    # \"Hey, can you pick me up at 5pm today?\",\n",
    "    # \"URGENT: Your account will be suspended unless you verify your details NOW\",\n",
    "    # \"Thanks for the meeting today, let's schedule the next one for next week\",\n",
    "    # \"Congratulations! You've won a prize! Call this number to claim it\"\n",
    "]\n",
    "\n",
    "print(\"Testing pipeline with different examples:\")\n",
    "print()\n",
    "\n",
    "for i, example in enumerate(test_examples, 1):\n",
    "    print(f\"\\n***Example {i}:\")\n",
    "    result = spam_classifier_pipeline(example, k=3)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "AaL0bg3io8a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Interactive Testing\n",
      "\n",
      "***Testing with k=5\n",
      "\n",
      "***Classifying: 'Win a free iPhone! Click here now!'\n",
      "\n",
      "***Using top-5 nearest neighbors\n",
      "\n",
      "***Prediction: SPAM\n",
      "\n",
      "***Top neighbors:\n",
      "1. Label: spam | Score: 0.8633\n",
      "   Message: FREE entry into our £250 weekly competition just text the word WIN to 80086 NOW. 18 T&C www.txttowin...\n",
      "\n",
      "2. Label: spam | Score: 0.8604\n",
      "   Message: FREE MESSAGE Activate your 500 FREE Text Messages by replying to this message with the word FREE For...\n",
      "\n",
      "3. Label: spam | Score: 0.8604\n",
      "   Message: FREE MESSAGE Activate your 500 FREE Text Messages by replying to this message with the word FREE For...\n",
      "\n",
      "4. Label: spam | Score: 0.8511\n",
      "   Message: U have won a nokia 6230 plus a free digital camera. This is what u get when u win our FREE auction. ...\n",
      "\n",
      "5. Label: spam | Score: 0.8507\n",
      "   Message: TheMob>Yo yo yo-Here comes a new selection of hot downloads for our members to get for FREE! Just cl...\n",
      "\n",
      "***To test with different inputs:\n",
      "1. Change 'user_text' variable above\n",
      "2. Change 'k_value' for different number of neighbors\n",
      "3. Re-run this cell\n"
     ]
    }
   ],
   "source": [
    "# Interactive testing - user có thể thay đổi text và k value\n",
    "print(\"***Interactive Testing\")\n",
    "print()\n",
    "\n",
    "# Người dùng có thể thay đổi các giá trị này để test với các ví dụ khác nhau\n",
    "user_text = \"Win a free iPhone! Click here now!\"\n",
    "k_value = 5\n",
    "\n",
    "print(f\"***Testing with k={k_value}\")\n",
    "result = spam_classifier_pipeline(user_text, k=k_value)\n",
    "\n",
    "print(\"***To test with different inputs:\")\n",
    "print(\"1. Change 'user_text' variable above\")\n",
    "print(\"2. Change 'k_value' for different number of neighbors\")\n",
    "print(\"3. Re-run this cell\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
